{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":82019,"databundleVersionId":8932715,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/harsimransinghdalal/factors-to-predict-disease-risk-in-individuals?scriptVersionId=185913915\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-06-28T12:43:49.659012Z","iopub.execute_input":"2024-06-28T12:43:49.659462Z","iopub.status.idle":"2024-06-28T12:43:49.66579Z","shell.execute_reply.started":"2024-06-28T12:43:49.659429Z","shell.execute_reply":"2024-06-28T12:43:49.664568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load datasets\ntrain_df = pd.read_csv('/kaggle/input/thapar-summer-school-2024-competition-2/train.csv/train.csv')\ntest_df = pd.read_csv('/kaggle/input/thapar-summer-school-2024-competition-2/test.csv/test.csv')\nsample_submission_df = pd.read_csv('/kaggle/input/thapar-summer-school-2024-competition-2/sample_submission (1).csv')\n\n# Display the first few rows of the datasets\nprint(\"Train Dataset:\")\nprint(train_df.head())\nprint(\"\\nTest Dataset:\")\nprint(test_df.head())\nprint(\"\\nSample Submission:\")\nprint(sample_submission_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-06-28T12:43:51.419325Z","iopub.execute_input":"2024-06-28T12:43:51.41971Z","iopub.status.idle":"2024-06-28T12:43:51.548533Z","shell.execute_reply.started":"2024-06-28T12:43:51.41968Z","shell.execute_reply":"2024-06-28T12:43:51.547293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling missing values\n# Fill missing values in numeric columns with mean\nnumeric_columns = train_df.select_dtypes(include=['number']).columns\ntrain_df[numeric_columns] = train_df[numeric_columns].fillna(train_df[numeric_columns].mean())\ntest_df[numeric_columns] = test_df[numeric_columns].fillna(test_df[numeric_columns].mean())\n\n# Fill missing values in categorical columns with mode\ncategorical_columns = train_df.select_dtypes(include=['object']).columns\ncategorical_columns = categorical_columns.drop('NObeyesdad')\ntrain_df[categorical_columns] = train_df[categorical_columns].fillna(train_df[categorical_columns].mode().iloc[0])\ntest_df[categorical_columns] = test_df[categorical_columns].fillna(test_df[categorical_columns].mode().iloc[0])\n\n# Combine train and test data for consistent label encoding\ncombined_df = pd.concat([train_df[categorical_columns], test_df[categorical_columns]])\n\n# Encoding categorical variables\nlabel_encoders = {}\nfor col in categorical_columns:\n    le = LabelEncoder()\n    combined_df[col] = le.fit_transform(combined_df[col])\n    train_df[col] = combined_df[:len(train_df)][col]\n    test_df[col] = combined_df[len(train_df):][col]\n    label_encoders[col] = le\n\n# Encode the target variable\ntarget_le = LabelEncoder()\ntrain_df['NObeyesdad'] = target_le.fit_transform(train_df['NObeyesdad'])\nlabel_encoders['NObeyesdad'] = target_le\n\n# Splitting features and target variable from training data\nX_train = train_df.drop(columns=['NObeyesdad'])\ny_train = train_df['NObeyesdad']\n\n# Split the data into training and validation sets\nX_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\n# Scale the data\nscaler = StandardScaler()\nX_train_split = scaler.fit_transform(X_train_split)\nX_val = scaler.transform(X_val)\nX_train = scaler.fit_transform(X_train)\ntest_df = scaler.transform(test_df)\n\n# Initialize the models\nxgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\nrf_model = RandomForestClassifier(random_state=42)\nlgb_model = LGBMClassifier(random_state=42,verbosity=-1)\n\n# Create a voting classifier\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('xgb', xgb_model),\n        ('rf', rf_model),\n        ('lgb', lgb_model)\n    ],\n    voting='soft'\n)\n\n# Train the voting classifier\nvoting_clf.fit(X_train_split, y_train_split)\n\n# Validate the model\ny_val_pred = voting_clf.predict(X_val)\naccuracy = accuracy_score(y_val, y_val_pred)\nprint(f'Validation Accuracy: {accuracy:.4f}')\n\n# Train the voting classifier on the entire training set\nvoting_clf.fit(X_train, y_train)\n\n# Make predictions on the test set\ntest_predictions = voting_clf.predict(test_df)\n\n# Map predictions back to original labels\ntest_predictions_labels = label_encoders['NObeyesdad'].inverse_transform(test_predictions)\n\n# Prepare the submission file\nsubmission_df = pd.DataFrame({\n    'id': sample_submission_df['id'],\n    'NObeyesdad': test_predictions_labels\n})\n\n# Save the submission file\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T12:43:54.223877Z","iopub.execute_input":"2024-06-28T12:43:54.224309Z"},"trusted":true},"execution_count":null,"outputs":[]}]}